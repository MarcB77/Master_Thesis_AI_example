{"cells":[{"cell_type":"markdown","source":["# Installs op cluster:\n\ntorch\nclick==8.0.4\nkeras\ntensorflow\nspacy \nscispacy\nnltk \ntorch"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5411a91f-4189-4a91-8e7b-606129f7c1df"}}},{"cell_type":"markdown","source":["# Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be23ad15-e50f-44ac-ab27-8b37d541575c"}}},{"cell_type":"code","source":["import pandas as pd\npd.options.mode.chained_assignment = None # Set warnings uit\nimport numpy as np\nimport os\nimport operator\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.window import Window\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom tqdm.keras import TqdmCallback\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils import resample\n\nimport keras\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten, Embedding, GlobalAveragePooling1D, Input, concatenate, LSTM\nfrom tensorflow.keras.optimizers import Adam, SGD\n\n# NLP libraries\nimport spacy\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom scispacy.abbreviation import AbbreviationDetector\nimport string\nimport tensorflow\nimport click\nfrom keras.preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences\n\nfrom IPython.display import clear_output\n\n%matplotlib inline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4284678d-77e0-471b-8348-15611fe857f8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Dataframes init"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3edf2c2-3fbd-49ef-9f0d-994384293d55"}}},{"cell_type":"code","source":["# Koppel ndAttributen\ndef koppel_ndAttributen(NASA_CONTROLE):\n    df_controle = spark.sql(\"select * from default._ndoutputattributen_25_3_2022\") # Verander deze met Vernieuwde lijst?\n    nd_controle = df_controle.join(NASA_CONTROLE, [\"gtin\"])\n    nd_controle = nd_controle.filter(nd_controle.glnAH == True)\n    print(\"Amount of rows: \",(nd_controle.count(),\"Amount of columns: \",len(nd_controle.columns))) # Shape of dataframe\n    return nd_controle\n\ndef koppel_ndAttributen_new_products(NASA_CONTROLE):\n    df_controle = spark.sql(\"select * from ndoutputattributen.ndoutputattributen\") # Verander deze met Vernieuwde lijst?\n    nd_controle = df_controle.join(NASA_CONTROLE, [\"gtin\"])\n    nd_controle = nd_controle.filter(nd_controle.glnAH == True)\n    print(\"Amount of rows: \",(nd_controle.count(),\"Amount of columns: \",len(nd_controle.columns))) # Shape of dataframe\n    return nd_controle\n\ndef koppel_preprocessed_text(df_geformateerd, DF_NLP_spark):\n    df_geformateerd = spark.createDataFrame(df_geformateerd)\n    DF_merged_ = (DF_NLP_spark.join(df_geformateerd, [\"gtin\", \"gln\",\"glnAH\"], how=\"left\"))  # verander hier table naar bovengenoemde attribuut, zie notebook --> Functions\n        \n    DF_merged_ = DF_merged_.drop_duplicates()\n    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n    DF_merged = DF_merged_.toPandas()\n    print(\"Shape van dataframe: \",(DF_merged[\"gln\"].count(),len(DF_merged.columns)))\n    return DF_merged\n    \ndef fix_empty_fields(DF_merged, ATTRIBUUT):\n    # Fix empty fields van class_num\n    print(\"null entries = \", DF_merged[\"class_num_\"+str(ATTRIBUUT)].isna().sum())\n    DF_merged = DF_merged[DF_merged[\"class_num_\"+str(ATTRIBUUT)].notna()]\n    return DF_merged\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff155539-0067-41f5-a5de-c5b3b160d16d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Functions Preprocessing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a44458ad-2096-491f-9019-62247f8850d8"}}},{"cell_type":"code","source":["def get_unique_classes(df, attribuut=\"ndTypeOfGrain\"):\n    unique_classes = []\n    for item in df[attribuut]:\n        unique_classes.append(item)\n        if item == '':\n            unique_classes.append(\"EMPTY\")\n    unique_classes = list(set(unique_classes))\n \n    # Split op -> |\n    items = []\n    for item in unique_classes:\n        try:\n            _item = item.split(\"|\")\n            items.append(_item)\n        except:\n            pass\n    items_flat = [item for sublist in items for item in sublist]\n    unique_items = sorted(list(set(items_flat))) # met set unique classes eruit halen, daarna weer terug naar een list\n    #unique_items.remove(\"\") # Haal de empty toegewezen class eruit\n    #print(unique_items, \"\\n\\nunique classes: \", len(unique_items))\n    return unique_items\n \ndef one_hot_encoder(df_unique, unique_items, attribuut=\"ndTypeOfGrain\"):\n    #one-hot encoder,\n    for index,item in enumerate(unique_items):\n        df_unique[str(unique_items[index])] = df_unique[attribuut].str.contains(pat = str(unique_items[index]))\n        df_unique[str(unique_items[index])] = df_unique[str(unique_items[index])]*1    # Convert boolean naar integer voor een NN model later.  \n    return df_unique\n \ndef create_overig(df_unique,ATTRIBUUT,unique_items, Overig_threshold):\n    all_classes = unique_items\n    df_unique_classes_only = df_unique[all_classes]\n    lijst_kleine_classes = []\n    grote_classes = []\n    for column in all_classes:\n        #print(column,(df_unique[column] == 1).sum())\n        if (df_unique[column] == 1).sum() <= Overig_threshold:\n            lijst_kleine_classes.append(column)\n        else:\n            grote_classes.append(column)\n    print(\"Kleine classes die onder overige moeten: \",lijst_kleine_classes)\n    \n    for index, class_ in enumerate(lijst_kleine_classes):\n        if index==0:\n            df_unique['overig'] = np.where(df_unique[class_] == 1, 1, 0)\n        else:\n            df_unique['overig'] = np.where(df_unique[class_] == 1, 1, df_unique['overig'])\n    print(\"Grotere classes: \", grote_classes)\n    grote_classes.append('overig')\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    \n    for index,item in enumerate(grote_classes):\n        df_encoded = pd.DataFrame(encoder.fit_transform(df_unique[[str(grote_classes[index])]]).toarray().astype(int))\n        df_unique[str(grote_classes[index])] = df_encoded.values.tolist()     \n    df_unique = df_unique.drop(columns=lijst_kleine_classes, axis=1)\n    return df_unique, grote_classes\n  \ndef classes_to_array(df_unique, attribuut=\"ndTypeOfGrain\"):\n    True_class = []\n    class_soort = []\n    df_classes_only = df_unique.drop(df_unique.columns[[0,1,2,3]], axis=1) # Drop eerste 4 columns van hierboven\n \n    for class_item in df_classes_only:\n        True_class.append(df_classes_only[class_item].sum())\n        class_soort.append(class_item)\n \n    classes_available = dict(zip(class_soort, True_class))\n    classes_available = dict( sorted(classes_available.items(), key=operator.itemgetter(1),reverse=True))\n \n    df_unique['Aantal_classes'] = df_unique[class_soort].sum(axis=1)\n    df_only_classes = df_unique[class_soort]\n    df_unique['class_num_'+attribuut] = df_only_classes.values.tolist()\n    df_unique['class_names_'+attribuut] = [class_soort for row in range(len(df_unique))]\n    return df_unique, classes_available\n \ndef without_all_classes(df_unique, attribuut=\"ndTypeOfGrain\"): \n    df_without_all_classes = df_unique[['gtin','gln','glnAH',str(attribuut),'Aantal_classes', 'class_num_'+str(attribuut),'class_names_'+str(attribuut)]].copy()\n    return df_without_all_classes\n    \ndef save_table(df_small, ATTRIBUUT):\n    # Save Table\n    spark_df = spark.createDataFrame(df_small)\n    file_name = '_'+ATTRIBUUT+'_ML_inc_overig'\n    spark.sql(\"use mltrainingdata\")\n    x = 'drop table if exists ' + file_name\n    spark.sql(x)\n    spark_df.write.mode(\"overwrite\").saveAsTable(file_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8adb43d-9bc7-4ae9-8cc1-00b00b3ccaa1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Plot functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bb38aa3-af71-4f99-96b4-b65d887c00b6"}}},{"cell_type":"code","source":["def plot_class_distributie(df_unique, attribuut=\"ndTypeOfGrain\"):\n    MAX_LENGTH = df_unique['Aantal_classes'].max()\n    sns.histplot(df_unique['Aantal_classes'], bins=MAX_LENGTH, kde=True, element=\"step\", color='navy')\n    plt.xlabel(\"Amount classes in product, dataset \"+attribuut)\n    plt.xlim(1,MAX_LENGTH+1)\n    combinations_count = list(df_unique['Aantal_classes'].value_counts())\n    plt.ylim(0, int(combinations_count[0])+100) # customize the size of the figure by the max value of bin 1\n    print(df_unique['Aantal_classes'].value_counts())\n\ndef distribution_of_unique_classes_2(classes_available, attribuut=\"ndTypeOfGrain\"):\n    print(classes_available)\n    plt.bar(range(len(classes_available)), list(classes_available.values()), tick_label = list(classes_available.keys()))\n    plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2d444a0-21e1-4ae3-a329-57e347106780"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create 'Others/Overige'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"277cb554-3df8-4292-a5dd-d945142b38d3"}}},{"cell_type":"code","source":["def create_overige_class(nd_controle, ATTRIBUUT,Classes_threshold, Overig_threshold):\n    multi_label_flag = False\n    from sklearn.preprocessing import OneHotEncoder\n    df = nd_controle.toPandas() # clear dataframe\n    df = df.replace(r'^\\s*$', 'EMPTY', regex=True)\n    Class_verdeling = df[ATTRIBUUT].value_counts().reset_index()\n    print(\"ATTRIBUUT: \", ATTRIBUUT)\n    #print(\"Verhouding van classes: \\n\",Class_verdeling)\n    unique_items = get_unique_classes(df,ATTRIBUUT)\n\n    # Split colomn into classes  / Split de column van het attribuut in classes\n    df_unique = df[[\"gtin\",\"gln\",\"glnAH\",ATTRIBUUT]]\n    if len(unique_items) >=Classes_threshold: # check if Attribuut has more than 10 classes\n        df_geformateerd = one_hot_encoder(df_unique, unique_items, ATTRIBUUT)\n        \n#         useless_df, classes_available_plot = classes_to_array(df_geformateerd, ATTRIBUUT)\n#         distribution_of_unique_classes_2(classes_available_plot,ATTRIBUUT)\n        \n        df_geformateerd, grote_classes = create_overig(df_geformateerd, ATTRIBUUT,unique_items, Overig_threshold)\n        df_geformateerd, classes_available = classes_to_array(df_geformateerd, ATTRIBUUT)\n        multi_label_flag = True\n    else:\n        df_geformateerd = one_hot_encoder(df_unique, unique_items, ATTRIBUUT)\n        df_geformateerd, classes_available = classes_to_array(df_geformateerd, ATTRIBUUT)\n        distribution_of_unique_classes_2(classes_available,ATTRIBUUT)\n        multi_label_flag = False\n\n    print(\"null entries = \", df_geformateerd[\"class_num_\"+str(ATTRIBUUT)].isna().sum())\n    return df_geformateerd,multi_label_flag,classes_available"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"084609ca-bf2d-40c1-ac2b-c9d31cd9d915"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Balance dataset if necessary"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ef5ece5-5585-4b4d-9c68-6c673b4e358b"}}},{"cell_type":"code","source":["def balance_multi_label(UPSAMPLE_Classes, y_train, X_train, classes, Upsample_size, balance_set):\n    print(\"\\n----------- BALANCING \", balance_set, \" -----------\")\n    for class_ in UPSAMPLE_Classes:\n        # UP-sample op xtrain en niet in testset en validatieset, zo komen dezelfde producten niet in alle sets.\n        df_minority_y = y_train.loc[y_train[class_].astype(str)==str(classes[1])]\n        samples_available = len(df_minority_y)\n        print(\"\\nSamples available \",class_ ,samples_available, \"\\nSearching for: \", str(classes[1]))\n        if samples_available < Upsample_size:\n            X_train_no_duplicates = X_train[~X_train.index.duplicated(keep='first')]\n\n            # Upsample minority class\n            y_train_upsample = resample(df_minority_y, \n                                             replace=True,     # sample with replacement\n                                             n_samples=int(Upsample_size-samples_available),    # to match majority class\n                                             random_state=123) # reproducible results\n            X_train_upsample = X_train_no_duplicates.loc[y_train_upsample.index.tolist()]\n            #X_train = X_train.loc[y_train.index.tolist()]\n            print(\"Shape before upsampling minority class \", len(y_train), len(X_train))\n            y_train = pd.concat([y_train, y_train_upsample])\n            X_train = pd.concat([X_train, X_train_upsample])\n            #print(len(df_minority_y), len(X_train_no_duplicates))\n            print(\"Add Extra samples Y_train | X_Train \", len(y_train_upsample),len(X_train_upsample))\n            print(\"New shape \", len(y_train), len(X_train))\n                        \n    return X_train, y_train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"239a6677-6ecb-418f-946d-e44f02c5a042"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def balance_single_label(DF_merged, upsample_classes_list, Upsample_size):\n    for class_ in upsample_classes_list:\n        # UP-sample op xtrain en niet in testset en validatieset, zo komen dezelfde producten niet in alle sets.\n        df_minority = DF_merged.loc[DF_merged[class_] == 1]\n        samples_available = len(df_minority)\n        print(\"Samples available = \",samples_available)\n        \n        if samples_available < Upsample_size:\n\n            # Upsample minority class\n            upsampled_df = resample(df_minority, \n                                             replace=True,     # sample with replacement\n                                             n_samples=int(Upsample_size-samples_available),    # to match majority class\n                                             random_state=123) # reproducible results\n            DF_merged = pd.concat([DF_merged, upsampled_df])\n                       \n    return DF_merged"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f97c158f-71c9-4264-b5b9-41c33ba43eb3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Under construction\ndef imbalance_check_single_label(classes_available, DF_merged, balance_threshold_single_label):\n    upsample_classes_list = []\n    for key, value in classes_available.items():\n        if value < balance_threshold_single_label:\n            upsample_classes_list.append(key)\n    if len(upsample_classes_list) > 0:\n        print(\"\\nDeze classes zijn laag in sample, en worden gebalanceerd: \", upsample_classes_list)\n        \n        print(\"Dataframe length, VOOR balancing\", len(DF_merged))\n        DF_merged = balance_single_label(DF_merged, upsample_classes_list, upsample_single_label)\n        print(\"Dataframe length, NA balancing\", len(DF_merged),\"\\n\")\n        \n    return DF_merged"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f142d7d4-2d19-4061-b286-eecec2203d11"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def equal_classes_downsample(classes_available, DF_merged):\n    print(\"Shape van dataframe: \",(DF_merged[\"gln\"].count(),len(DF_merged.columns)))\n    downsample_df = pd.DataFrame()\n    value_classes = []\n    classes = []\n    for key, value in classes_available.items():\n        classes.append(key)\n    for class_ in classes:\n        df_minority = DF_merged.loc[DF_merged[class_] == 1]\n        value_classes.append(len(df_minority))\n    downsample_value = np.amin(value_classes)\n    print(\"downsampling each class to: \", downsample_value)\n    \n    for class_ in classes:\n        df_minority = DF_merged.loc[DF_merged[class_] == 1]\n        samples_available = len(df_minority)\n        print(class_+\" samples available: \",samples_available)\n        \n        # Upsample minority class\n        sample_df = resample(df_minority, \n                                         replace=True,     # sample with replacement\n                                         n_samples=int(downsample_value),    # to match majority class\n                                         random_state=123) # reproducible results\n        downsample_df = pd.concat([downsample_df, sample_df])\n                       \n    print(\"Shape of downsampled dataframe: \", len(downsample_df), \"\\n\")\n    return downsample_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3aa295d7-5bcd-4459-81ab-de7ee9ce7bf3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Natural Language Processing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90b75a78-a03a-4279-a9d9-5ff45e9fe4d9"}}},{"cell_type":"code","source":["class Natural_Language_Processing_nd:\n    def __init__(self, df_NLP, input_features, filename):\n        self.df_NLP = df_NLP\n        self.input_features = input_features\n        self.filename = filename\n        \n    def prepare_downloads(self):\n        !python -m spacy download nl_core_news_lg\n        !python -m spacy download en_core_web_lg\n        nltk.download('all')\n        nlp_dutch = spacy.load(\"nl_core_news_lg\")\n        nlp_en = spacy.load(\"en_core_web_lg\")\n        return nlp_dutch, nlp_en\n\n################################### Preprocessing functions ####################################\n    def empty_fields_fix(self, df_NLP):\n        df_NLP['all_text_features'] = '' #create empty column in dataframe\n        for feature in self.input_features:\n            print('Feature '+feature+' ',(df_NLP[feature].isna()).sum()) # CHECK how many empty fields there are, since this causes errors\n            df_NLP[feature] = df_NLP[feature].fillna('')\n            df_NLP['all_text_features'] += df_NLP[feature]+' '\n        return df_NLP\n\n    def lower_case(self, df_NLP): # Lowercase\n        df_NLP['lowercase'] = df_NLP['all_text_features'].str.lower()\n        return df_NLP\n\n    def remove_punctuation(self, df_NLP): # Remove punctuation and symbols\n        PUNCT_TO_REMOVE = string.punctuation\n        print('Punctuation that will be removed: ',PUNCT_TO_REMOVE)\n        df_NLP[\"no_punctuation\"] = df_NLP[\"lowercase\"].apply(lambda text: text.translate(str.maketrans('', '', PUNCT_TO_REMOVE)))\n        #display(df_NLP.head())\n        return df_NLP\n\n    def stopwords_removal(self, df_NLP):   # Remove stopwords\n        from nltk.corpus import stopwords\n        nltk.download('stopwords')\n        STOPWORDS = stopwords.words('dutch') + [\"empty_cell\"]\n        df_NLP[\"NO_stopwords\"] = df_NLP['no_punctuation'].apply(lambda text: \" \".join([word for word in str(text).split() if word not in STOPWORDS]))   # Remove STOPWORDS\n        #display(df_NLP.head())\n        return df_NLP\n\n    def most_frequent_words(self, df_NLP):   # Find most frequent words in product sentences\n        from collections import Counter\n        cnt = Counter()\n        for text in df_NLP[\"NO_stopwords\"].values:\n            for word in text.split():\n                cnt[word] += 1      \n        most_common = cnt.most_common(10)\n        print(\"Most common words:\\n\", most_common)\n        return most_common\n\n    def lemmatization(self, df_NLP, nlp_dutch, nlp_en):  # Bring words back to one basis form\n        print(\"Lemmmatization, takes a while..\")\n        df_NLP[\"lemmatized_DUTCH\"] = df_NLP['NO_stopwords'].apply(lambda x: \" \".join([y.lemma_ for y in nlp_dutch(x)]))\n#         df_NLP[\"lemmatized_DUTCH_EN\"] = df_NLP['lemmatized_DUTCH'].apply(lambda x: \" \".join([y.lemma_ for y in nlp_en(x)]))\n        print(\"Lemmatization done!\")\n        return df_NLP\n\n    def tokenization(self, df_NLP): # Split sentence in to tokens\n        df_NLP[\"tokenized_DUTCH\"] = df_NLP[\"lemmatized_DUTCH\"].apply(nltk.word_tokenize)\n#         df_NLP[\"tokenized_EN_DUTCH\"] = df_NLP[\"lemmatized_DUTCH_EN\"].apply(nltk.word_tokenize)\n        return df_NLP\n\n    def text_2_sequence(self, df_NLP):   # Text to numerical data, understandable for Machine learning\n        tokenizer = Tokenizer() #init\n\n        df_NLP['length_tokenized'] = df_NLP['tokenized_DUTCH'].str.len()\n        MAX_LEN = df_NLP['length_tokenized'].max()\n        print(\"Max length of dutch: \",MAX_LEN)\n        sequences = tokenizer.fit_on_texts(df_NLP[\"tokenized_DUTCH\"])\n        sequences = tokenizer.texts_to_sequences(df_NLP[\"tokenized_DUTCH\"])\n        padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN)\n        df_NLP[\"padded_seq_\"] = padded_sequences.tolist()\n        \n#         df_NLP['length_tokenized_EN'] = df_NLP['tokenized_EN_DUTCH'].str.len()\n#         MAX_LEN = df_NLP['length_tokenized_EN'].max()\n#         print(\"Max length dutch+en: \",MAX_LEN)\n#         sequences = tokenizer.fit_on_texts(df_NLP[\"tokenized_EN_DUTCH\"])\n#         sequences = tokenizer.texts_to_sequences(df_NLP[\"tokenized_EN_DUTCH\"])\n#         padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN)\n#         df_NLP[\"padded_seq__EN\"] = padded_sequences.tolist()\n        return df_NLP, tokenizer\n\n    def save_NLP_table(self, df_NLP):   # Save Table, such that we can use it for the neural network later on.\n        spark_df = spark.createDataFrame(df_NLP)\n        spark.sql(\"use default\")\n        to_drop_str = 'drop table if exists ' + self.filename\n        spark.sql(to_drop_str)\n        spark_df.write.mode(\"overwrite\").saveAsTable(self.filename)\n        print(\"Saved as: _nd_Modelling_NLP\")\n        return spark_df\n\n    def save_tokenizer(self,tokenizer):\n        tokenizer_json = tokenizer.to_json()\n        dbutils.fs.put(\"/FileStore/nd_MODELS/Tokenizer.json\", tokenizer_json)\n\n    \n    def NLP_Loop(self, overwrite_tokenizer, overwrite_NLP_table):\n        nlp_dutch, nlp_en = self.prepare_downloads() #nlp_en can be added if needed\n        clear_output(wait=True)\n        df_NLP = self.empty_fields_fix(self.df_NLP)\n        df_NLP = self.lower_case(df_NLP)\n        df_NLP = self.remove_punctuation(df_NLP)\n        df_NLP = self.stopwords_removal(df_NLP)\n        most_common = self.most_frequent_words(df_NLP)\n        df_NLP = self.lemmatization(df_NLP, nlp_dutch, nlp_en)\n        df_NLP = self.tokenization(df_NLP)\n        df_NLP, tokenizer = self.text_2_sequence(df_NLP)\n        if overwrite_NLP_table==True:\n            spark_df = self.save_NLP_table(df_NLP)\n        if overwrite_tokenizer==True:\n            self.save_tokenizer(tokenizer)\n        return spark_df, tokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8970f377-1b81-4801-ba94-960d50a10aaf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Functions Training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c9a1647-9504-4565-b85d-99ea733b9a2c"}}},{"cell_type":"markdown","source":["## Correlatie matrix, Pass Numerical Features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7acc6053-cf94-4211-b52f-b55f26bccc57"}}},{"cell_type":"code","source":["def Correlation_matrix(DF_merged, ATTRIBUUT, show_nan_plot=True, show_correlation_matrix=True):\n    # numerieke features met empty entries geven error op de correlatie tabel\n    plt.rcParams.update({'font.size': 11})\n    NUMERIEKE_nutrienten = [\"FIBER\",\"SUGAR\",\"FAT\",\"FASAT\",\"CHOAVL\",\"PROTEINE\",\"SALT\",\"ENER_KJ\",\"ENER_Kcal\"]\n    DF_merged[\"CLASS_\"+str(ATTRIBUUT)] = DF_merged[\"class_num_\"+str(ATTRIBUUT)].apply(np.argmax)\n    \n    nan_list_numeriek = []\n    nan_list_labels = []\n    for nutrient in NUMERIEKE_nutrienten:\n        DF_merged[nutrient] = DF_merged[nutrient].astype(float)\n        #print(nutrient, DF_merged[nutrient].isna().sum())\n        nan_list_numeriek.append(DF_merged[nutrient].isna().sum())\n        nan_list_labels.append(nutrient)\n        #DF_merged[nutrient] = DF_merged[nutrient].fillna(0)\n        DF_merged[nutrient+\"_was_missing\"] = np.where(DF_merged[nutrient].isnull(), 1, 0)\n        if nutrient == \"SUGAR\":\n            DF_merged[\"SUGAR_IS_ZERO\"] = np.where(DF_merged[nutrient]== 0.0, 1, 0)\n        # REPLACE the null now by a mean\n        DF_merged[nutrient] = DF_merged[nutrient].fillna(DF_merged[nutrient].mean()) # mean or median\n    if show_nan_plot == True: \n        fig, ax = plt.subplots(figsize =(16, 9))\n        bars = ax.barh(nan_list_labels, nan_list_numeriek)        \n        ax.set_title('Null values in nutritional attributes.\\nBased on Food products.',\n                     loc ='center', )\n        ax.bar_label(bars, label_type='center', fontsize=20)\n        print(bars, type(bars))\n        # Show Plot\n        plt.show()\n    # string features omzetten naar numerieke classes\n    from sklearn import preprocessing\n    le = preprocessing.LabelEncoder()\n    le.fit(DF_merged['Fiber_UOM'])\n    DF_merged['Fiber_UOM_class']=le.transform(DF_merged['Fiber_UOM'])\n    le.fit(DF_merged['NBQ_UOM'])\n    DF_merged['NBQ_UOM_class']=le.transform(DF_merged['NBQ_UOM'])\n    le.fit(DF_merged['SUGAR_UOM'])\n    DF_merged['SUGAR_UOM_class']=le.transform(DF_merged['SUGAR_UOM'])\n\n    #display(DF_merged.head())\n    if show_correlation_matrix==True:\n        df_correlatie = pd.DataFrame(DF_merged,columns=[\"FIBER\",\"SUGAR\",\"FAT\",\"FASAT\",\"CHOAVL\",\"PROTEINE\",\"SALT\",\"ENER_KJ\",\"Fiber_UOM_class\",\"SUGAR_UOM_class\",\"NBQ_UOM_class\",'CLASS_'+str(ATTRIBUUT)])\n        f = plt.figure(figsize=(15,11))\n        corrMatrix = df_correlatie.corr()\n        sns.heatmap(corrMatrix, annot=True, fmt='.2f', cmap='Blues',annot_kws={\"fontsize\":16})\n        plt.show()\n    return DF_merged"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10250145-61f4-47f6-aa1b-a90ac1c83556"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Prepare Train, Test en validatie data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"722a3bd1-435c-439f-98cc-99184ccc21f8"}}},{"cell_type":"code","source":["def Numerical_features(ATTRIBUUT, DF_merged):\n    #Numerical_features = DF_merged[['Fiber_UOM_class','NBQ_UOM_class','SUGAR_UOM_class']]\n    Numerical_features = DF_merged[['Fiber_UOM_class','NBQ_UOM_class','SUGAR_UOM_class']]\n    # Welke numerical features wil je bij het attribuut?\n    if ATTRIBUUT == \"ndFiberIndex\":\n        Numerical_features = DF_merged[['FIBER','FIBER_was_missing','Fiber_UOM_class','PROTEINE']] # ndFiberIndex\n    if ATTRIBUUT == \"ndAddedSugar\":\n        Numerical_features = DF_merged[['SUGAR_was_missing','SUGAR_UOM_class','SUGAR']]\n    if ATTRIBUUT == \"ndFreeOfAlcohol\":\n        Numerical_features = DF_merged[['Fiber_UOM_class','NBQ_UOM_class','SUGAR_UOM_class']]\n    if ATTRIBUUT == \"ndTypeOfGrain\":\n        Numerical_features = ['FIBER']\n    if ATTRIBUUT == \"ndAnimalSpecies\":\n        Numerical_features = ['FIBER']\n    return Numerical_features\n\ndef Formateer_Features(DF_merged, ATTRIBUUT):\n    X_train_text_DF = DF_merged['padded_seq_'].tolist()\n    X_train_text = torch.FloatTensor(X_train_text_DF)\n    X_train_text = X_train_text.numpy()\n    vocab_size = np.amax(X_train_text)+1\n\n    # TARGET\n    ytrain = DF_merged['class_num_'+ATTRIBUUT].tolist()\n    ytrain_tensor = torch.FloatTensor(ytrain)\n    ytrain = ytrain_tensor.numpy()\n    print(\"Vocabular size: \", vocab_size)\n    return vocab_size, X_train_text, ytrain"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ddb0a48-d827-4ffd-b44b-d149c22d4a59"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def split_train_test(xtrain, ytrain):\n    from sklearn.model_selection import train_test_split\n    Xtrain, Xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.3, random_state = 42)\n    Xval, Xtest, yval, ytest = train_test_split(Xval, yval, test_size=0.5, random_state = 42)\n    return Xtrain, ytrain, Xval, yval, Xtest, ytest\n\ndef unzip_sets(Xtrain, Xval, Xtest):\n    Xtrain_num = Xtrain[:,0]\n    Xtrain_text = Xtrain[:,1]\n    Xval_num = Xval[:,0]\n    Xval_text = Xval[:,1]\n    Xtest_num = Xtest[:,0]\n    Xtest_text = Xtest[:,1]\n    return Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text\n  \ndef total_split(DF_merged, numerieke_features, ATTRIBUUT):\n    from sklearn.model_selection import train_test_split\n    X = DF_merged[['gtin','gln','Lemmatized',ATTRIBUUT,'padded_seq_','class_names_'+ATTRIBUUT]]\n    X2 = DF_merged[numerieke_features]\n    X = X.merge(X2, left_index=True, right_index=True, how='inner')\n    targets = DF_merged['class_names_'+ATTRIBUUT].tolist()\n    targets = targets[0]\n    y = DF_merged[targets]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                        test_size=0.2, random_state=1)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n                                                      test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n\n    print(X_train.shape, X_test.shape, X_val.shape)\n    print(y_train.shape, y_test.shape, y_val.shape)\n    return  X_train, X_val, X_test, y_train, y_val, y_test, targets\n\ndef to_tensor(Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text):\n    Xtrain_num = Xtrain_num.tolist()\n    Xtrain_num = torch.FloatTensor(Xtrain_num)\n    Xtrain_num = Xtrain_num.numpy()\n\n    Xval_num = Xval_num.tolist()\n    Xval_num = torch.FloatTensor(Xval_num)\n    Xval_num = Xval_num.numpy()\n\n    Xtest_num = Xtest_num.tolist()\n    Xtest_num = torch.FloatTensor(Xtest_num)\n    Xtest_num = Xtest_num.numpy()\n\n    Xtrain_text = Xtrain_text.tolist()\n    Xtrain_text = torch.FloatTensor(Xtrain_text)\n    Xtrain_text = Xtrain_text.numpy()\n\n    Xval_text = Xval_text.tolist()\n    Xval_text = torch.FloatTensor(Xval_text)\n    Xval_text = Xval_text.numpy()\n\n    Xtest_text = Xtest_text.tolist()\n    Xtest_text = torch.FloatTensor(Xtest_text)\n    Xtest_text = Xtest_text.numpy()\n    return  Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text\n  \ndef multiple_classes_PREP(numerical_ftrs, X_train, X_test, X_val, y_train, y_test, y_val, target):\n    X_train_num = X_train[numerical_ftrs].to_numpy()\n    X_test_num = X_test[numerical_ftrs].to_numpy()\n    X_val_num = X_val[numerical_ftrs].to_numpy()\n\n    X_train_text = X_train['padded_seq_'].tolist()\n    X_train_text = torch.FloatTensor(X_train_text)\n    X_train_text = X_train_text.numpy()\n    \n    X_test_text = X_test['padded_seq_'].tolist()\n    X_test_text = torch.FloatTensor(X_test_text)\n    X_test_text = X_test_text.numpy()\n    \n    X_val_text = X_val['padded_seq_'].tolist()\n    X_val_text = torch.FloatTensor(X_val_text)\n    X_val_text = X_val_text.numpy()\n    \n    # TARGET\n    ytrain = y_train[target].tolist()\n    ytrain_tensor = torch.FloatTensor(ytrain)\n    ytrain = ytrain_tensor.numpy()\n\n    ytest = y_test[target].tolist()\n    ytest_tensor = torch.FloatTensor(ytest)\n    ytest = ytest_tensor.numpy()\n\n    yval = y_val[target].tolist()\n    yval_tensor = torch.FloatTensor(yval)\n    yval = yval_tensor.numpy()\n\n    return X_train_num, X_test_num, X_val_num, X_train_text, X_test_text, X_val_text, ytrain, ytest, yval\n  \ndef single_prep(numerieke_features, DF_merged, ATTRIBUUT):\n    Numerical_features = DF_merged[numerieke_features]\n\n    X_train_num = Numerical_features.to_numpy()\n\n    vocab_size, X_train_text, ytrain = Formateer_Features(DF_merged, ATTRIBUUT)\n    print(\"Numerical Features: \", Numerical_features.columns)\n    print(\"Shape Numerical Features: \", X_train_num.shape)\n    print(\"Shape Text Features: \", X_train_text.shape)\n    print(\"Shape Targets: \", ytrain.shape)\n\n    X_train = np.array(list(zip(X_train_num, X_train_text)))\n    Xtrain, ytrain, Xval, yval, Xtest, ytest = split_train_test(X_train, ytrain)\n    print(Xtrain.shape, Xtest.shape, Xval.shape)\n    print(ytrain.shape, ytest.shape, yval.shape)\n    \n    Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text = unzip_sets(Xtrain, Xval, Xtest)\n    Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text = to_tensor(Xtrain_num, Xtrain_text, Xval_num, \n                                                                                Xval_text, Xtest_num, Xtest_text)# To Tensor format\n    return ytrain, yval, ytest, Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text\n  \ndef single_prep_controle(numerieke_features, DF_merged, ATTRIBUUT):\n    Numerical_features = DF_merged[numerieke_features]\n\n    X_train_num = Numerical_features.to_numpy()\n\n    vocab_size, X_train_text, ytrain = Formateer_Features(DF_merged, ATTRIBUUT)\n    print(\"Numerical Features: \", Numerical_features.columns)\n    print(\"Shape Numerical Features: \", X_train_num.shape)\n    print(\"Shape Text Features: \", X_train_text.shape)\n    print(\"Shape Targets: \", ytrain.shape)\n\n    return X_train_num, X_train_text, ytrain, vocab_size"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a161c1e-3f65-44b6-bc83-27c24fbafa97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Model metrics, statistics and plots"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fd304ee-4556-4e15-825d-58a6fc8c8ca2"}}},{"cell_type":"code","source":["def predictions(model):\n    yhat =  model.predict(Xtest, verbose=1)\n    yhat2 = np.round(yhat, 0)\n    model.evaluate(Xtest, ytest)\n\n    list_ytest = []\n    list_yhat = []\n    for idx in range(len(yhat2)):\n        list_ytest.append(np.argmax(ytest[idx]))\n        list_yhat.append(np.argmax(yhat2[idx]))\n    return list_ytest, list_yhat, yhat2\n\ndef accuracy_report(y_test, y_pred, macro_f1_list):\n    print(\"Confusion Matrix:\\n**************************\\n \",\n         confusion_matrix(y_test, y_pred),\"\\n**************************\\n\")\n    print(\"Accuracy: \",\n         accuracy_score(y_test, y_pred)*100)\n    print(\"Classification report: \",\n         classification_report(y_test, y_pred))\n    report = classification_report(y_test, y_pred, output_dict=True)\n    macro_f1 = report['macro avg']['f1-score']\n    macro_f1_list.append(macro_f1)\n    return macro_f1_list\n    \ndef plot_confusion_matrix(DF_merged, cf_matrix, ATTRIBUUT):\n    unique_classes = DF_merged[\"class_num_\"+str(ATTRIBUUT)].tolist()\n    unique_classes = [item for item in unique_classes]\n    unique_classes = unique_classes[0]\n    print(\"unique amount of classes: \",len(unique_classes), unique_classes)\n    group_names = []\n    for i in range(len(unique_classes)):\n        for class_ in unique_classes: \n            group_names.append(class_)\n\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix.flatten()/np.sum(cf_matrix)]\n\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n              zip(group_counts,group_percentages)]\n    \n    class_names= DF_merged[\"class_names_\"+str(ATTRIBUUT)].tolist()\n    \n    \n    labels = np.asarray(labels).reshape(len(unique_classes), len(unique_classes))\n    s = sns.heatmap(cf_matrix, xticklabels=class_names[0], yticklabels=class_names[0], annot=labels, fmt='', cmap='Blues')\n    s.set(xlabel='Predicted', ylabel='True label')\n    \n    s.set_title(\"Attribute:\\n \"+str(ATTRIBUUT)+\"\\n\"+str(' - '.join(class_names[0])))\n    plt.show()\n    \ndef plot_training(acc, val_acc, NAME='Accuracy'):\n    EPOCH = len(acc)\n    for param in ['figure.facecolor', 'axes.facecolor', 'savefig.facecolor']:\n        plt.rcParams[param] = '1.0'#'#212946'  # bluish dark grey\n    for param in ['text.color', 'axes.labelcolor', 'xtick.color', 'ytick.color']:\n        plt.rcParams[param] = '0.3'  # very light grey\n    epochs_range = np.arange(1,EPOCH+1)\n    plt.plot(epochs_range, acc, 'g', label='Train '+str(NAME))\n    plt.plot(epochs_range, val_acc, 'b', label='Val '+str(NAME))\n    plt.title('Training and Validation \\n '+str(NAME)+' of '+str(ATTRIBUUT))\n    plt.xlabel('Epochs')\n    plt.ylabel(str(NAME))\n    plt.legend()\n    plt.show()\n    \ndef predictions_2inputs(model, Xtest_num,Xtest_text, ytest):\n    yhat =  model.predict([Xtest_num, Xtest_text], verbose=1)\n    yhat2 = np.round(yhat, 0)\n    model.evaluate([Xtest_num, Xtest_text], ytest)\n\n    list_ytest = []\n    list_yhat = []\n    for idx in range(len(yhat2)):\n        list_ytest.append(np.argmax(ytest[idx]))\n        list_yhat.append(np.argmax(yhat2[idx]))\n    return list_ytest, list_yhat, yhat2\n\ndef plot_multiple_training(acc, val_acc, class_names, ATTRIBUUT, type_):\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(25,14))\n    EPOCH = len(acc[0])\n    for param in ['figure.facecolor', 'axes.facecolor', 'savefig.facecolor']:\n        plt.rcParams[param] = '1.0'\n    for param in ['text.color', 'axes.labelcolor', 'xtick.color', 'ytick.color']:\n        plt.rcParams[param] = '0.3'  \n    epochs_range = np.arange(1,EPOCH+1)\n    for index, class_acc in enumerate(acc):\n        ax1.plot(epochs_range, class_acc, label=str(class_names[index]))\n    for index, class_val_acc in enumerate(val_acc):\n        ax2.plot(epochs_range, class_val_acc, label=str(class_names[index]))\n    ax1.set_title('Training \\n '+type_+' of '+str(ATTRIBUUT))\n    ax2.set_title('Validation \\n '+type_+' of '+str(ATTRIBUUT))\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel(type_)\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel(type_)\n    ax1.legend()\n    ax2.legend()\n    plt.show()\n    \ndef plot_single_label_training(model, model_history, Xtest_num, Xtest_text, ytest, ATTRIBUUT, DF_merged):\n    print(model_history.history.keys())\n \n    list_ytest, list_yhat, yhat2 = predictions_2inputs(model, Xtest_num, Xtest_text, ytest)\n\n    plot_confusion_matrix(DF_merged, confusion_matrix(list_ytest, list_yhat), ATTRIBUUT)\n    macro_f1_list = []\n    macro_f1_list = accuracy_report(list_ytest, list_yhat,macro_f1_list)\n    print(macro_f1_list)\n    plot_training(model_history.history['accuracy'], model_history.history['val_accuracy'],NAME='Accuracy')\n    plot_training(model_history.history['loss'], model_history.history['val_loss'],NAME='Loss')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf0f7284-82b1-4b35-8c90-74702b0c5e62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Vocabulary Size"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d021163-8d64-468c-99da-411a9fd9379e"}}},{"cell_type":"code","source":["def get_vocabulary_size(DF_ATTRIBUUT):\n    vocab = DF_ATTRIBUUT['padded_seq_'].tolist()\n    vocab = torch.FloatTensor(vocab)\n    vocab = vocab.numpy()\n    vocab_size = np.amax(vocab)+1\n    print(\"Vocab size: \", vocab_size)\n    return vocab_size"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df6deb09-bf22-4dd8-bc8e-0ce4b2e33128"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Neural network"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b328e67-8901-4c26-84f5-e7ba4fdd4f03"}}},{"cell_type":"code","source":["class all_models:\n    def __init__(self, Xtrain_num, Xtrain_text, vocab_size, ytrain):\n        self.Xtrain_num = Xtrain_num\n        self.Xtrain_text = Xtrain_text\n        self.vocab_size = vocab_size\n        self.ytrain = ytrain\n        self.input_NUM = Input(shape=self.Xtrain_num.shape[1]) # Numerieke features\n        self.input_TEXT = Input(shape=self.Xtrain_text.shape[1]) # Text features\n        \n    def LOAD_MODEL(self):\n        emb = Embedding(int(self.vocab_size), output_dim=20,input_length=self.Xtrain_text.shape[1])(self.input_TEXT)\n        #lstm = LSTM(8, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n        fltn = Flatten()(emb)\n        x = concatenate([fltn, self.input_NUM])\n\n        x = Dense(30, activation='relu')(x)\n        x = Dropout(0.5)(x)\n        x = Dense(20, activation='relu')(x)\n        x = Dropout(0.5)(x)\n        x = Dense(self.ytrain.shape[1], activation='softmax')(x)\n\n        model = Model(inputs=[self.input_NUM , self.input_TEXT], outputs=[x])\n        model.summary()\n\n        optimizer = SGD(learning_rate=0.01)\n        #model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n        return model\n\n    def LOAD_MODEL_2(self):\n        emb = Embedding(int(self.vocab_size), output_dim=4,input_length=self.Xtrain_text.shape[1])(self.input_TEXT)\n        #lstm = LSTM(8, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n        emb = Flatten()(emb)\n        input_NUM = Dropout(0.3)(self.input_NUM)\n        emb = Dropout(0.3)(emb)\n        x = concatenate([emb, input_NUM])\n        x = Dense(30, activation='relu')(x)\n        x = Dense(50, activation='relu')(x)\n        x = Dense(20, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        x = Dense(self.ytrain.shape[1], activation='softmax')(x)\n\n        model = Model(inputs=[self.input_NUM , self.input_TEXT], outputs=[x])\n        model.summary()\n\n        optimizer = SGD(learning_rate=0.008)\n        #model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n        return model\n\n    def LOAD_MODEL_3(self):\n        emb = Embedding(int(self.vocab_size), output_dim=4,input_length=self.Xtrain_text.shape[1])(self.input_TEXT)\n        #lstm = LSTM(8, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n        emb = Flatten()(emb)\n        input_NUM = Dropout(0.3)(self.input_NUM)\n        emb = Dropout(0.3)(emb)\n        x = concatenate([emb, input_NUM])\n        x = Dense(100, activation='relu')(x)\n        x = Dense(50, activation='relu')(x)\n        x = Dense(100, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        x = Dense(self.ytrain.shape[1], activation='softmax')(x)\n\n        model = Model(inputs=[self.input_NUM , self.input_TEXT], outputs=[x])\n        model.summary()\n\n        optimizer = SGD(learning_rate=0.008)\n        #model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n        return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b800dba-34ce-4a63-a928-74aaa07fd036"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Train model multi-label or mutli-class (Single Label)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df7a5768-2e4b-474c-ac82-c95791a1b95f"}}},{"cell_type":"code","source":["def TRAINING_multi_label(X_train, X_val, X_test, y_train, y_val, y_test, numerical_ftrs, vocab_size, ATTRIBUUT, SAVE_MODEL):\n    total_pred = []\n    total_test = []\n    \n    train_accuracies = []\n    val_accuracies = []\n    \n    train_loss = []\n    val_loss = []\n    \n    class_names = []\n    macro_f1_list= []\n    print(\"**************** MULTI LABEL MODEL ***************\\n\")\n    print(\"Save all models = \",SAVE_MODEL,\"\\nNumerieke features = \", numerical_ftrs, \"\\nAttribuut = \", ATTRIBUUT )\n    \n    for (target, data) in tqdm(y_train.iteritems()):\n        print('\\n',target)\n        Xtrain_num, Xtest_num, Xval_num, Xtrain_text, Xtest_text, Xval_text, ytrain, ytest, yval = multiple_classes_PREP(numerical_ftrs, X_train, X_test, X_val, y_train, y_test, y_val, target)\n        \n        print(\"X-Train | X-Validation | X-Test ## Numerical shape\", Xtrain_num.shape,  Xval_num.shape,  Xtest_num.shape)\n        print(\"X-Train | X-Validation | X-Test ## Textual shape\", Xtrain_text.shape, Xval_text.shape, Xtest_text.shape)\n        print(\"Y-Train | Y-Validation | Y-Test ## TARGET shape\", ytrain.shape, yval.shape, ytest.shape)\n        models = all_models(Xtrain_num, Xtrain_text, vocab_size, ytrain)\n        model = models.LOAD_MODEL_3()\n\n        model_history = model.fit([Xtrain_num,Xtrain_text], ytrain, epochs=30, verbose=2, validation_data=([Xval_num,Xval_text], yval), shuffle=True)\n        list_ytest, list_yhat, yhat2 = predictions_2inputs(model, Xtest_num, Xtest_text, ytest)\n        macro_f1_list = accuracy_report(list_ytest, list_yhat, macro_f1_list) \n\n        train_accuracies.append(model_history.history['accuracy'])\n        val_accuracies.append(model_history.history['val_accuracy'])\n        train_loss.append(model_history.history['loss'])\n        val_loss.append(model_history.history['val_loss'])\n        \n        class_names.append(target)\n        if SAVE_MODEL == True:\n            model.save(\"/FileStore/nd_MODELS/\"+str(ATTRIBUUT)+\"/model_\"+str(target)+\".h5\")\n            dbutils.fs.cp(\"file:/FileStore/nd_MODELS/\"+str(ATTRIBUUT)+\"/model_\"+str(target)+\".h5\", \"dbfs:/FileStore/nd_MODELS/\"+str(ATTRIBUUT)+\"/model_\"+str(target)+\".h5\") \n            display(dbutils.fs.ls(\"/FileStore/nd_MODELS/\"+str(ATTRIBUUT)+\"/model_\"+str(target)+\".h5\"))\n    print(class_names, macro_f1_list)\n    return train_accuracies, val_accuracies, train_loss, val_loss, class_names, model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c3b5904-b3cf-4511-affe-5a3b11ae946d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def TRAINING_single_label( Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text, vocab_size, EPOCHS, SAVE_MODEL):\n    print(\"**************** SINGLE LABEL MODEL ***************\\n\")\n    models = all_models(Xtrain_num, Xtrain_text, vocab_size, ytrain)\n    model = models.LOAD_MODEL_2()\n    model_history = model.fit([Xtrain_num,Xtrain_text], ytrain, epochs=EPOCHS, verbose=2, validation_data=([Xval_num,Xval_text], yval), shuffle=True, batch_size=32) # Start training the model --> Singel label\n    \n    if SAVE_MODEL == True:\n        model.save(\"/FileStore/nd_MODELS/model_\"+str(ATTRIBUUT)+\".h5\")\n        dbutils.fs.cp(\"file:/FileStore/nd_MODELS/model_\"+str(ATTRIBUUT)+\".h5\", \"dbfs:/FileStore/nd_MODELS/model_\"+str(ATTRIBUUT)+\".h5\") \n        display(dbutils.fs.ls(\"/FileStore/nd_MODELS/model_\"+str(ATTRIBUUT)+\".h5\"))\n    return model, model_history"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee99436c-036f-4199-b403-8685c20eb19f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Functions Prediction Notebooks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d01a90fb-0d62-4cef-a368-89595da098cf"}}},{"cell_type":"code","source":["def get_targets(DF_merged, ATTRIBUUT):\n    TARGETS = DF_merged['class_names_'+ATTRIBUUT].tolist()\n    TARGETS = TARGETS[0]\n    print(\"Targets: \",TARGETS)\n    for index, target in enumerate(TARGETS):\n        if target == 'EMPTY':\n            index_empty = index  \n    print(\"Index of Empty: \", index_empty)\n    return index_empty, TARGETS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c88b8172-6cfe-4f42-81ee-83cd2637764f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def load_numerical_features(ATTRIBUUT):\n    spark.sql(\"use nieuwe_data\")\n    num_features = spark.sql(\"select * from nd_models__numerical_features\")\n    num_features = num_features.toPandas()\n    attribuut_rows = num_features.loc[num_features['ndAttribuut'] == ATTRIBUUT]\n    attribuut_rows = spark.createDataFrame(attribuut_rows)\n    w = Window.partitionBy('ndAttribuut').orderBy(col('date').desc())\n    df = (attribuut_rows .withColumn('rank',F.row_number().over(w)))\n    df = (df .filter(df['rank'] == 1).drop('rank'))\n    num_features = df.toPandas()\n    \n    numerieke_features = num_features['Num_Features'].tolist()\n    numerieke_features = numerieke_features[0]\n    if numerieke_features == ['']:\n        numerieke_features = []\n    print(\"Numerieke Features: \", numerieke_features)\n    return numerieke_features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8e5342b-93b3-4f63-a97d-46ae90c84006"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def predictions_to_dataframe(DF_merged, ATTRIBUUT, list_ytest, list_yhat):\n    controle_tabel = DF_merged[['gtin','gln','Lemmatized',str(ATTRIBUUT)]]\n    labels = DF_merged['class_names_'+str(ATTRIBUUT)].tolist()\n    labels = labels[0]\n\n    controle_tabel['TRUE'] = list_ytest\n    controle_tabel['PREDICTED'] = list_yhat\n\n    total_labels = []\n    product_labels= []\n    for index in controle_tabel['PREDICTED']:\n        product_labels.append(labels[index])\n        #total_labels.append(product_labels)\n    controle_tabel['PREDICTED Labels'] = product_labels\n    print(controle_tabel[ATTRIBUUT].isna().sum())\n    return controle_tabel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b490b8ac-1839-4730-8ba0-827880ded09e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Create Controle Attribuut\ndef ControleAttribuut(df):\n    NASA = spark.sql(\"select NASA_NR, GLN as gln, ARTIKEL_EAN as gtin, MERKNAAM, SCHAPSTICKER_OMSCHRIJVING, ASSGROEP_OMSCHRIJVING from StandaardTabellen.NasaStandaardTabel\")\n \n    GLNAH = spark.read.format('delta').load('/mnt/Prd_adls/Conformed/TIMS/TradeItem/ahTradeItem/Data/').filter('__DeletedFlag == 0')\n    GLNAH = GLNAH.select(col(\"gtin\"), col(\"gln\"), col(\"glnAH\"))\n \n    NASA = NASA.join(GLNAH, ['gtin', 'gln'], how=\"left\")\n    NASA = NASA.filter(NASA.glnAH == True)\n    NASA = NASA.drop('glnAH')\n \n    ControleAttribuut = NASA.join(df, ['gtin', 'gln'], how=\"left\")\n   \n#     for i in ControleAttribuut.columns:\n#         ControleAttribuut = ControleAttribuut.withColumn(i, regexp_replace(i, '\\n', ''))\n        \n    return ControleAttribuut"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a2d6180-fc3d-40b9-8bf0-4f5709611432"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def unequal_and_equal_table(controle_tabel, ATTRIBUUT):\n    NLP_unlemmatized = spark.sql(\"select gtin,gln, NO_stopwords as UnLemmatized from default._nd_Modelling_NLP\")\n#     NLP_unlemmatized = NLP_unlemmatized.toPandas()\n#     NLP_unlemmatized['UnLemmatized'] = NLP_unlemmatized['UnLemmatized'].str.replace(',','')\n#     NLP_unlemmatized['UnLemmatized'] = NLP_unlemmatized['UnLemmatized'].str.replace('.','')\n#     NLP_unlemmatized = spark.createDataFrame(NLP_unlemmatized)\n    \n    UNEQUAL_TABLE = controle_tabel.loc[~(controle_tabel['TRUE'] == controle_tabel['PREDICTED'])]\n    EQUAL_TABLE = controle_tabel.loc[(controle_tabel['TRUE'] == controle_tabel['PREDICTED'])]\n    UNEQUAL_TABLE.drop(columns=['TRUE','PREDICTED'], axis=1, inplace=True)\n    EQUAL_TABLE.drop(columns=['TRUE','PREDICTED'], axis=1, inplace=True)\n    # add the right NASA columns to dataframe\n    \n    UNEQUAL_TABLE_spark = spark.createDataFrame(UNEQUAL_TABLE)\n    UNEQUAL_TABLE_spark = UNEQUAL_TABLE_spark.join(NLP_unlemmatized, ['gtin','gln'], how = \"left\")\n    UNEQUAL_TABLE_spark = ControleAttribuut(UNEQUAL_TABLE_spark)\n    UNEQUAL_TABLE_spark = UNEQUAL_TABLE_spark.filter(UNEQUAL_TABLE_spark[ATTRIBUUT].isNotNull())\n    UNEQUAL_TABLE_spark = UNEQUAL_TABLE_spark.drop_duplicates()\n    \n    EQUAL_TABLE_spark = spark.createDataFrame(EQUAL_TABLE)\n    EQUAL_TABLE_spark = EQUAL_TABLE_spark.join(NLP_unlemmatized, ['gtin','gln'], how = \"left\")\n    EQUAL_TABLE_spark = ControleAttribuut(EQUAL_TABLE_spark)\n    EQUAL_TABLE_spark = EQUAL_TABLE_spark.filter(EQUAL_TABLE_spark[ATTRIBUUT].isNotNull())\n    EQUAL_TABLE_spark = EQUAL_TABLE_spark.drop_duplicates()\n    return EQUAL_TABLE_spark, UNEQUAL_TABLE_spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50a183a2-769c-4a13-86b6-37d3c6e3583d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Edit predictions, important part!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3531b53-fecd-4448-a37d-7d754e1bf51b"}}},{"cell_type":"code","source":["def predictions_to_right_format(total_test,total_pred, ATTRIBUUT, index_empty, controle_tabel, DF_merged):\n    total_test_T = np.array(total_test)\n    total_test_T = total_test_T.T\n    total_test_T = total_test_T.tolist()\n    total_pred_T = np.array(total_pred)\n    total_pred_T = total_pred_T.T\n    total_pred_T = total_pred_T.tolist()\n\n    empty_pred_dummy = []\n    complete_empty_dummy = []\n    for index, value in enumerate(total_pred_T[0]):\n        if index == index_empty:\n            empty_pred_dummy.append(1)\n        else:\n            empty_pred_dummy.append(0)\n        complete_empty_dummy.append(0)\n    print(\"empty dummy\", empty_pred_dummy)\n    print(\"complete empty dummy\", complete_empty_dummy)\n        \n    for index, row_pred in enumerate(total_pred_T):\n        if row_pred == complete_empty_dummy:\n            total_pred_T[index] = empty_pred_dummy # Voorbeeld van een EMPTY list\n\n    # Wanneer er empty + graan aanwezig is, verwijder dan empty. Alleen nodig bij multi-label models\n    prediction_new = []\n    for prediction in total_pred_T:\n        if prediction.count(1) >1 and prediction[index_empty]==1:\n            prediction_cp = prediction\n            prediction_cp[index_empty] = 0\n        else:\n            prediction_cp = prediction\n        prediction_new.append(prediction_cp)\n\n    controle_tabel['TRUE'] = total_test_T\n    controle_tabel['PREDICTED'] = prediction_new #total_pred_T\n    controle_tabel_T = controle_tabel[['gtin','gln','Lemmatized',ATTRIBUUT,'TRUE','PREDICTED']]\n\n    # Place labels in column\n    labels = DF_merged['class_names_'+ATTRIBUUT].tolist()\n    labels = labels[0]\n\n    total_labels = []\n    for row in controle_tabel_T['PREDICTED']:\n        product_labels = []\n        for index, label in enumerate(row):\n            if label == 1:\n                product_labels.append(labels[index])\n        total_labels.append(product_labels)\n    controle_tabel_T['PREDICTED Labels'] = total_labels\n    return controle_tabel_T"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68139049-fdda-4e85-9914-f53bc5ed6cb2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Write numerical features to a text file, so we know later (predictions) which numerical features has been used."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02ad028c-84cf-41a7-9691-d658b61772e5"}}},{"cell_type":"code","source":["def save_numerical_features_(SAVE_MODEL, ATTRIBUUT, numerical_features):\n    from datetime import datetime\n    from pytz import timezone\n    \n    if SAVE_MODEL == True:\n        spark.sql(\"use nieuwe_data\")\n        spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\", \"true\")\n        #spark.sql(\"drop table if exists nd_MODELS__numerical_features\")\n        \n        columns = ['ndAttribuut', 'Num_Features', 'Date']\n        amsterdam = timezone('Europe/Amsterdam')\n        date = datetime.now(amsterdam).strftime('%Y-%m-%d')\n        newRow = spark.createDataFrame([(ATTRIBUUT,numerical_features,date)], columns)\n        #appended = df.union(newRow)\n        #appended.show()\n        newRow.write.mode(\"append\").saveAsTable(\"nd_MODELS__numerical_features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd7bfebd-00ca-458e-8e05-47ff903d7cfc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Functions_Finalized","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3325852567044182}},"nbformat":4,"nbformat_minor":0}
